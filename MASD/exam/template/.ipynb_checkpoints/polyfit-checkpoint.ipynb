{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template for Exercise 1 in the MASD 2018 exam\n",
    "\n",
    "This is a template for doing Exercise 1 of the MASD 2018 Exam. Please fill out the fields and function templates below, reading the data as below, in order to perform the exercise. Feel free to load extra functions, but note that built-in functions for doing the entire task are, as a general rule, not allowed.\n",
    "\n",
    "If you are unsure how to interpret the template, or what you are allowed to do, please contact us either by email or using the Absalon forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages -- feel free to add more, but recall that you should not use built-in \n",
    "# functions for the task at hand. If in doubt, ask us by email.\n",
    "import numpy as np\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we load the data to which we will fit a polynomial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('E1data.txt')\n",
    "xs = data[:,0]\n",
    "ys = data[:,1]\n",
    "plt.scatter(xs, ys, label='data')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 e:** In the box below, implement the computation of $\\textbf{w}$ for the dataset supplied above. Perform the computation for $d = 4$, and plot the corresponding function $f_\\mathbf{w}$ together with the data. Try different values of $d$. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1f:** In the box below, implement gradient descent to find the optimal $\\mathbf{w}$. Run it on the supplied dataset with $d=4$ and compare your solution to the optimal solution from e) by plotting them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyfit_gradient(xs, ys, d, w0):\n",
    "    # This function is meant to compute the gradient of E with respect to the variable vector w = [w1, ..., wd]^T\n",
    "    # Input: * the vector of data point x-coordinates, xs\n",
    "    #        * the vector of data point y-coordinates, ys\n",
    "    #        * the wanted polynomial degree d\n",
    "    #        * The current value w0 of w\n",
    "    # Return: The gradient grad_w of E with respect to w at w=w0\n",
    "                                            \n",
    "    return grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyfitGD(xs, ys, d):\n",
    "    # A template function for performing gradient descent over w to minimize E\n",
    "    # Input: * the vector of data point x-coordinates, xs\n",
    "    #        * the vector of data point y-coordinates, ys\n",
    "    #        * the wanted polynomial degree d\n",
    "        \n",
    "    # Initialize at random\n",
    "    np.random.seed(1) # Fixed seed for reproducibility; please don't change\n",
    "    w = np.random.randn(d)\n",
    "    \n",
    "    # Set learning rate (you may want to play with this)\n",
    "    learningrate = 0.00001\n",
    "    \n",
    "    # Setting parameters for convergence check \n",
    "    # (you may want to play with the latter two)\n",
    "    num_iter = 1                   # This is the variable that will keep track of the number of iterations\n",
    "    convergence = 0                # This is the variable that will keep track of whether we have converged\n",
    "    max_iter = 10000               # We stop the algorithm after this many iterations\n",
    "    tolerance = 0.01               # We conclude convergence when the magnitude of the gradient\n",
    "                                   # is less than the tolerance\n",
    "    \n",
    "    while convergence == 0:\n",
    "        ####################### MISSING PART ############## \n",
    "        # Compute gradient and take a step in the direction it dictates\n",
    "        \n",
    "        \n",
    "        \n",
    "        ####################### MISSING PART ##############        \n",
    "        # Check for convergence -- you need to fill in the computation of the norm of the gradient at the current location\n",
    "        num_iter = num_iter + 1      \n",
    "        cur_grad_norm = ##### MISSING\n",
    "        \n",
    "        if cur_grad_norm < tolerance:\n",
    "            convergence = 1\n",
    "            print('converged')\n",
    "        elif num_iter > max_iter:\n",
    "            convergence = 1 \n",
    "            print('reached maximum nr of iterations')\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run your code to compute the optimal $\\textbf{w}$, and compare it to your optimal value of $\\mathbf{w}$ from above, both by comparing the resulting vectors, and by plotting the two functions together, along with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
