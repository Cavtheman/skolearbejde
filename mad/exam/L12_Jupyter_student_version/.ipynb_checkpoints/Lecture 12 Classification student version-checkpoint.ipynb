{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 12: Classification\n",
    "\n",
    "Today's demo contains\n",
    "* A kNN example\n",
    "* A Logistic regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load packages as usual\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import matplotlib.cm as cm\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "# Manipulating figure sizes\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (15,7)\n",
    "matplotlib.rc('font', size=15)\n",
    "matplotlib.rc('axes', titlesize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 1: k-Nearest Neighbor classification\n",
    "\n",
    "We consider the medical application of diagnosing Parkinson's disease from a person's voice.  We consider the data from Little et al, which can be obtained from the well-known UCI benchmark repository (see Frank and Asuncion).\n",
    "\n",
    "The data were collected from 31 people, 23 suffering from Parkinson's disease.  Several voice recordings of these people were processed. Each line in the data files corresponds to one recording.  The first 22 columns are features derived from the recording, including minimum, average and maximum vocal fundamental frequency, several measures of variation in fundamental frequency, several measures of variation in amplitude, two measures of ratio of noise to tonal components in the voice status, two nonlinear dynamical complexity measures, a measure called signal fractal scaling exponent, as well as nonlinear measures of fundamental frequency variation (Little et al, Frank and Asuncion).  The last column is the target label indicating whether the subject is healthy (0) or suffers from Parkinson's disease (1).\n",
    "\n",
    "**References:**\n",
    "M.A.~Little, P.E.~McSharry, E.J.~Hunter, J.~Spielman and L.O.~Ramig, \\emph{Suitability of dysphonia measurements for telemonitoring of {P}arkinson's disease}, IEEE Transactions on Biomedical Engineering, Vol.~56, No.~4, pp.~1015--1022, 2009.\n",
    "A. Frank and A. Asuncion, \\emph{{UCI} Machine Learning Repository}, \\url{http://archive.ics.uci.edu/ml}, University of California, Irvine, School of Information and Computer Sciences, 2010.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Loading, processing and checking the data\n",
    "train = np.loadtxt('parkinsonsTrain.dt')\n",
    "test = np.loadtxt('parkinsonsTest.dt')\n",
    "trainlabels = train[:,-1]\n",
    "testlabels = test[:,-1]\n",
    "traindata = train[:,0:-1]\n",
    "testdata = test[:,0:-1]\n",
    "trainnum, dim = traindata.shape\n",
    "testnum, dim = testdata.shape\n",
    "print('N_train:', trainnum)\n",
    "print('N_test:', testnum)\n",
    "print('dim:', dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Visualize the dataset using PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def pca(data):\n",
    "    # Extract data dimensions\n",
    "    d, N = data.shape\n",
    "\n",
    "    # First, center the data\n",
    "    center = np.mean(data, 1)\n",
    "    centers = np.matlib.repmat(center, N, 1)\n",
    "    data_cent = data - np.transpose(centers)\n",
    "    \n",
    "    # Compute covariance and its eigenvalues from centered data\n",
    "    Sigma = np.cov(data_cent)\n",
    "    evals, evecs = np.linalg.eigh(Sigma)\n",
    "    \n",
    "    # Return eigenvalues and eigenvectors and -- for the sake of the lecture -- also the centered data\n",
    "    return np.flip(evals,0), np.flip(evecs, 1), data_cent\n",
    "\n",
    "PCevals, PCevecs, data_cent = pca(traindata.T)\n",
    "\n",
    "# Next, project onto two first eigenvectors for visualization\n",
    "PC1 = PCevecs[:,0]\n",
    "PC2 = PCevecs[:,1]\n",
    "\n",
    "label1 = np.argwhere(trainlabels==1)\n",
    "label0 = np.argwhere(trainlabels==0)\n",
    "PC1projs = np.matmul(data_cent.T,PC1)\n",
    "PC2projs = np.matmul(data_cent.T,PC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(PC1projs[label0], PC2projs[label0], color='blue', label='Healthy')\n",
    "plt.scatter(PC1projs[label1], PC2projs[label1], color='red', label='Parkinsons')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Projection of Parkinsons dataset onto the first two PCs, colored by diagnosis')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, we make a simple implementation of kNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def knn(train, test, trainlabels, testlabels, k):\n",
    "    # Fill in\n",
    "    \n",
    "    return dmat, pred_classes, acc\n",
    "\n",
    "\n",
    "# Euclidean distance matrix:\n",
    "def dist_m(set1, set2):\n",
    "    set1num, dim = set1.shape\n",
    "    set2num, dim = set2.shape\n",
    "    D = np.zeros((set1num, set2num))\n",
    "    for i in range(set1num):\n",
    "        for j in range(set2num):\n",
    "            D[i,j] = np.linalg.norm(set1[i,:]-set2[j,:])\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Run kNN, testing on both the training data and the test data, for k=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Testing on training data:\n",
    "_, _, acc = knn(traindata, traindata, trainlabels, trainlabels, 1)\n",
    "print('Train Accuracy:', acc)\n",
    "\n",
    "# Testing on test data:\n",
    "_, _, acc = knn(traindata, testdata, trainlabels, testlabels, 1)\n",
    "print('Test Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Repeat the experiment for varying k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ks = range(1,16)\n",
    "acc_train = np.zeros(15)\n",
    "acc_test = np.zeros(15)\n",
    "for k in ks:\n",
    "    _,_,acc_train[k-1] = knn(traindata, traindata, trainlabels, trainlabels, k)\n",
    "    _,_,acc_test[k-1] = knn(traindata, testdata, trainlabels, testlabels, k)\n",
    "    \n",
    "plt.plot(acc_train, label='Training accuracy')\n",
    "plt.plot(acc_test, label='Testing accuracy')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 2: Logistic regression for classification\n",
    "\n",
    "We start out by loading and viewing the dataset (from http://www.ats.ucla.edu/stat/r/dae/logit.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Loading and structuring\n",
    "data = np.loadtxt('admission_dataset.txt')\n",
    "data_matrix = data[:,[0,1]]\n",
    "admission_labels = data[:,2]\n",
    "\n",
    "N, num_feat = data_matrix.shape\n",
    "colors = ['red', 'blue']\n",
    "admittance = ['admitted', 'not admitted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that before starting, we need to turn the admission labels (the classes) into +/- 1 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = ((admission_labels-0.5)*2).reshape(N,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Viewing as a scatter plot\n",
    "for i in range(N):\n",
    "    plt.scatter(data_matrix[i,0], data_matrix[i,1], color=colors[int(admission_labels[i])])\n",
    "    plt.xlabel('GPA')\n",
    "    plt.ylabel('GRE score')\n",
    "    \n",
    "plt.title('College admittance as a function of GPA and GRE score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our first building block is the logistic function; let's define and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return np.exp(x)/(1 + np.exp(x))\n",
    "\n",
    "# Plotting\n",
    "xs = np.linspace(-10,10,num=100)\n",
    "plt.plot(xs, logistic(xs))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Plot of logistic function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, let's implement the logistic regression loss function (for the training set (X, y)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_insample(X, y, w):\n",
    "    N, num_feat = X.shape\n",
    "    w = w.reshape(num_feat,1)\n",
    "    E = 0\n",
    "    for n in range(N):\n",
    "        xn = X[n,:].reshape(num_feat,1)\n",
    "        E += (1/N)*np.log(1/logistic(y[n]*np.matmul(w.T,xn)))\n",
    "    return E[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, we implement its gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_gradient(X, y, w):\n",
    "    N, num_feat = X.shape\n",
    "    w = w.reshape(num_feat,1)    \n",
    "    g = np.zeros(w.shape)\n",
    "    \n",
    "    for n in range(N):\n",
    "        xn = X[n,:].reshape(num_feat,1)\n",
    "        increment = ((-1/N)*y[n]*xn)*logistic(-y[n]*np.matmul(w.T,xn))\n",
    "        g += increment\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Having working functions for logistic log likelihood and logistic gradient, let's implement gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_reg(Xorig, y, max_iter, grad_thr):       \n",
    "    # X is an N by d data matrix of input values\n",
    "    # y is a N by 1 matrix of target values -1 and 1\n",
    "    num_pts, num_feat = Xorig.shape\n",
    "    onevec = np.ones((num_pts,1))\n",
    "    X = np.concatenate((onevec, Xorig), axis = 1)    \n",
    "    dplus1 = num_feat + 1\n",
    "        \n",
    "    # Initialize learning rate for gradient descent\n",
    "    learningrate = 0.1        \n",
    "    \n",
    "    # Initialize weights at time step 0    \n",
    "    w = 0.1*np.random.randn(num_feat + 1).reshape(num_feat+1,1)\n",
    "    \n",
    "    # Compute value of logistic log likelihood\n",
    "    value = logistic_insample(X,y,w)\n",
    "    \n",
    "    num_iter = 0  \n",
    "    convergence = 0\n",
    "    \n",
    "    # Keep track of function values\n",
    "    E_in = []\n",
    "    \n",
    "    while convergence == 0:\n",
    "        num_iter = num_iter + 1                        \n",
    "\n",
    "        # Compute gradient at current w      \n",
    "        g = logistic_gradient(X,y,w)\n",
    "       \n",
    "        # Set direction to move       \n",
    "        v = -g\n",
    "                     \n",
    "        # Update weights\n",
    "        w_new = w + learningrate*v\n",
    "       \n",
    "        # Check for improvement: Compute in-sample error for new w\n",
    "        cur_value = logistic_insample(X,y,w_new)\n",
    "        if cur_value < value:\n",
    "            w = w_new\n",
    "            value = cur_value\n",
    "            E_in.append(value)\n",
    "            # Adaptive learning rate: Increase learning rate when things go well\n",
    "            learningrate *=1.1\n",
    "        else:\n",
    "            # Adaptive learning rate: Decrease learning rate if no improvement\n",
    "            learningrate *= 0.9   \n",
    "            \n",
    "        # Determine whether we have converged: Is gradient norm below\n",
    "        # threshold, and have we reached max_iter?\n",
    "               \n",
    "        g_norm = np.linalg.norm(g)\n",
    "        if g_norm < grad_thr:\n",
    "            convergence = 1\n",
    "            print('converged')\n",
    "        elif num_iter > max_iter:\n",
    "            convergence = 1\n",
    "            print('reached maximum nr of iterations')\n",
    "           \n",
    "    return w, E_in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's run logistic regression on our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wopt, E = log_reg(data_matrix, y, 100000, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Having optimized the model, let's predict classes on the training set by computing class probabilities and thresholding them at 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def log_pred(Xorig, w):\n",
    "    # X is a d by N data matrix of input values    \n",
    "    num_pts, num_feat = Xorig.shape\n",
    "    w = w.reshape(num_feat+1,1)    \n",
    "    onevec = np.ones((num_pts,1))\n",
    "    X = np.concatenate((onevec, Xorig), axis = 1)\n",
    "    P = np.zeros(num_pts)\n",
    "    for n in range(num_pts):\n",
    "        xn = X[n,:].reshape(num_feat+1,1)   \n",
    "        P[n] = logistic(np.matmul(w.T,xn)) # Probability of having label +1\n",
    "        \n",
    "    Pthresh = np.round(P) #0/1 class labels\n",
    "    pred_classes = Pthresh*2-1\n",
    "    return P, pred_classes\n",
    "\n",
    "P, pred_classes = log_pred(data_matrix, wopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's evaluate the classification accuracy on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test = np.abs(pred_classes.reshape(N,1) - y)\n",
    "errors = np.sum(np.abs(pred_classes.reshape(N,1) - y))/2\n",
    "acc = (N-errors)/N\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "My classification accuracy is not great -- is this just not working? Look back at the input dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Viewing training set as a scatter plot\n",
    "plt.scatter(data_matrix[:,0], data_matrix[:,1], c=P, cmap=cm.jet)\n",
    "plt.xlabel('GPA')\n",
    "plt.ylabel('GRE score')\n",
    "plt.colorbar()\n",
    "    \n",
    "plt.title('Predicted College admittance probability as a function of GPA and GRE score')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
